{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled29.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdExPu2a4W1vtVFlpWb/cC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/AdvancedPytorch_Colab/blob/master/Pytorch%20Applstra_%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%921%E3%81%A4%E3%81%9A%E3%81%A4%E8%A1%A8%E7%A4%BA3.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KHgYAvVpn_B",
        "colab_type": "text"
      },
      "source": [
        "#予測結果を1つずつ表示する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVZc-9bMt0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjP9t3rDphbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pycolor:\n",
        "    BLACK = '\\033[30m'\n",
        "    RED = '\\033[31m'\n",
        "    GREEN = '\\033[32m'\n",
        "    YELLOW = '\\033[33m'\n",
        "    BLUE = '\\033[34m'\n",
        "    PURPLE = '\\033[35m'\n",
        "    CYAN = '\\033[36m'\n",
        "    WHITE = '\\033[37m'\n",
        "    RETURN = '\\033[07m' #反転\n",
        "    ACCENT = '\\033[01m' #強調\n",
        "    FLASH = '\\033[05m' #点滅\n",
        "    RED_FLASH = '\\033[05;41m' #赤背景+点滅\n",
        "    END = '\\033[0m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80OHu7Nxp2_G",
        "colab_type": "text"
      },
      "source": [
        "#Google colabをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvOx674M7-v",
        "colab_type": "code",
        "outputId": "9b1f51e5-80ad-4bea-b966-148cac64b360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "'''\n",
        "ファイル構成\n",
        "My Drive---Deep learning---applstra------train----appl\n",
        "                              |      |         |--stra\n",
        "                              |      |    \n",
        "                              |      |---val-------appl\n",
        "                              |                 |--stra\n",
        "                              |---applstra.pth\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvXG1608p7Hv",
        "colab_type": "text"
      },
      "source": [
        "#モデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofhExJn0NS0l",
        "colab_type": "code",
        "outputId": "5e784315-2c66-4513-824d-a5ec21857ad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# モデルの設定\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/applstra/applstra.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ypu_u_Bd-4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#モデルのサマリー（省略可）\n",
        "from torchsummary import summary\n",
        "summary(model_ft, (3, 224, 224))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtApBBavy4g4",
        "colab_type": "text"
      },
      "source": [
        "#画像とラベル表示のための関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvVtYkUOy5A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(Variable(image_tensor))\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #正解は青色、不正解は赤色で表示する\n",
        "    _, pred = torch.max(output, 1)\n",
        "    predicted_label = class_name[pred]\n",
        "\n",
        "    return(predicted_label)  #class_nameの番号で出力される\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    Precision  = TP/(FP + TP)\n",
        "    Recall = TP/(TP + FN)\n",
        "    Specificity = TN/(FP + TN)\n",
        "    F_value = (2*Recall*Precision)/(Recall+Precision)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ctcIUsdqDwf",
        "colab_type": "text"
      },
      "source": [
        "#メインプログラム\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SBzXxouqEJ1",
        "colab_type": "code",
        "outputId": "b5014155-ba3b-4dd1-933a-76fb32aff1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#ファイル名の取得\n",
        "image_path = glob.glob(\"/content/drive/My Drive/Deep_learning/applstra/val/*/*\")\n",
        "print(len(image_path))\n",
        "print(image_path)\n",
        "\n",
        "#対象画像のパスからラベルを抜き出す\n",
        "\n",
        "\n",
        "#ファイル名よりラベルを抜き出し、'class_name'と定義\n",
        "class_name = []\n",
        "class_path = glob.glob('/content/drive/My Drive/Deep_learning/applstra/val/*')\n",
        "for i in class_path:\n",
        "    class_name.append(os.path.basename(i))  \n",
        "print(class_name)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "127\n",
            "['/content/drive/My Drive/Deep_learning/applstra/val/appl/mitukoto--770x507.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/pd_3e7850611c4cf3ea9261cde0e41656c63703be8f.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/mitu--770x513.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/mrk003_1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/natsuakari.JPG', '/content/drive/My Drive/Deep_learning/applstra/val/appl/P2019047.JPG', '/content/drive/My Drive/Deep_learning/applstra/val/appl/pd_d81aa71d05b502c92a0b0bc2eb59b81060a74d20.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/PED_kajiraretaapple2_TP_V4.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ph00161251.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/photo-1505872342847-6dbb5e76cd31-768x512_1400x.progressive.jpeg.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ph_kikuchi_ringo02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ph1_komitsu.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/prm1908050003-p1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/picture_pc_8cc7dcb534e8285786c8f9ba03894a73.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/photo01.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ph_thumb.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/rgn1910240057-p1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/publicdomainq-0002607tcr.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/Red_Apple.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-all02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-1024x682.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-top.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-fuji.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-fuji1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo-tsugaru.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo08.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringokokuten.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo1_lp.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringo2-300x225.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sd1_2d9932c1aa71bb42067b3bb48e357bb1810a588c.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sankotoku.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sanfuji5kgy.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sd1_aad9f76074b7c12d4e0fa6d2b37aae827b04bbc7.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/ringohatake.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sd1_c8ab8625531af3bdb42243a9171e0cb567871f8b.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/shutterstock_19154770.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sunkakou10top.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sk0210062.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/shutterstock_252604903-690x510.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/shun_10_02_34.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/shun_10_02__icat-1100x594.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sunhuji--770x513.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/slider_02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/src_11192805.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/SRxArPn8Ta2F._UX970_TTW__.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/s_f3b2d3f354f6913add709a56c72d4ff2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/title-yukiguni-ringo.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/town20191011152132.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/sweet5kgy.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/tekateka-618x254.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/title-apple.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/title_main.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/top_main.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/top_img_sp.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/top.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/travel-aomori-photo01_240.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/unnamed-2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/X720.480_productmain_56573188_3817_orig.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/tpc-thi-lifewithwood-013-main.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/tsukijiichiba_204b09506.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/typhoonsunfuji-03_3L.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/uwasanosaika.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/w5v32h3snhoijow67i7b.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/wkredgoldm_002.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/appl/zwzccszsrd1gbc3a7xc9.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/pd_5269a90e65552f8123ea0351750f074f8afa1350.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/photo07.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/pd_5718cf7318eeaade3efbbcdc11f6e47d841efe96.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sd1_1567a4d71d4bf1ae380d801354c792930e316b05.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sales_natsuakari.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/res_mon_ichigofair.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sd1_456d214d259adf6f4d279b80e7d4ce70453d7484.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/picture_pc_ae52ebd8d46fd8c5e514706f5875bf67.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/pickup.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/pd_7fcf162e3bd4e72694f7c84c68bc2f0a78480147.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sd1_db1626e1bb33e3caa645872a853b1006bf849414.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/shironefruits04.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/slider2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/slproImg_201804251817490.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/shutterstock_410180086cs-1200x630.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/slider_181220.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/shironefruits06.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sp_image1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/smitigo28-top.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/slproImg_201804251819070.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/smitigo24-top.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sp-img005.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sp_sec01.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberry1634_01.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/sp_main.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/st600_2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberries-3431122_960_720-2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberry_002.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberry.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/Strawberries.JPG', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberry-wallpaper.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/strawberry_fv.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tbm_20190110122012.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tokusen02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top-22.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tbm_20191101184911.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/Strawberry_photo.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/ti1-1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/titleimg.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tochigi-ichigo_no_sato001.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tochigi_ichigonosato27.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top-conD01.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top2.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_strawberry_sp.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_about2new.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top-image.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/topimage_new01.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top-pic01-2018.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_on_img.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_img_ichigo2019.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_ichigo.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/top_img02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/udoki_u_20190302_02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/web_food_photo1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/zzzzz_zzzzz_744_496_keep_2a8bcf5fee5a0f427995fde52abebe4f90f66f00.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/tujyo02.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/v2_HO040_main1_3.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/trend_20190513183945-thumb-645xauto-157487.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/wst1908310018-p1.jpg', '/content/drive/My Drive/Deep_learning/applstra/val/stra/Yotuboshi11L.jpg']\n",
            "['appl', 'stra']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF4j8b2JE-EK",
        "colab_type": "text"
      },
      "source": [
        "・True positive (TN) <br>\n",
        "・False positive (FP) <br>\n",
        "・True negative (TN) <br>\n",
        "・False negative (FN) <br>\n",
        "\n",
        "1. Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "2. Precision = TP/(FP + TP) ※positive predictive value\n",
        "3. Recall = TP/(TP + FN)　※sensitivity\n",
        "4. Specificity = TN/(FP + TN)\n",
        "5. F_value = (2RecallPrecision)/(Recall+Precision)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okikEKlnrEP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TP = 0 #True positive\n",
        "FP = 0 #False positive\n",
        "TN = 0 #True negative\n",
        "FN = 0 #False negative\n",
        "\n",
        "\n",
        "for i in image_path:\n",
        "    image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "    image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "    predicted_label = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "    \n",
        "    print('Image: '+ image_name)\n",
        "    print('Label: '+ label)\n",
        "    print('Pred: '+ predicted_label)\n",
        "    showImage(i)  #画像を表示\n",
        "    print() #空白行を入れる\n",
        "    time.sleep(0.1)\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "'''\n",
        "    if label == class_name[pred]:\n",
        "        print(pycolor.BLUE+ 'Predicted: '+ class_name[pred] +pycolor.END)\n",
        "        ans = 'correct'\n",
        "    if label != class_name[pred]:\n",
        "        print(pycolor.RED+ 'Predicted: '+ class_name[pred] +pycolor.END)\n",
        "        ans = 'incorrect'\n",
        "'''\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdTpi1ASFDAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}